{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47d8b54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prabin_linux/ai/pytorch_phase1/venv_torch/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from utils.tensor_generator import tensor_gen, one_hot_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8389fe",
   "metadata": {},
   "source": [
    "USING SIGMOID IN PYTORCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a31092f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9975]])\n"
     ]
    }
   ],
   "source": [
    "ip_tensor = torch.tensor([[6]])\n",
    "sigmoid = nn.Sigmoid()\n",
    "output = sigmoid(ip_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5075dd48",
   "metadata": {},
   "source": [
    "USE SOFTMAX IN PYTORCH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d76fc397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5197e-05, 5.0325e-04, 1.1171e-02, 7.4689e-02, 3.7186e-03, 9.0990e-01]])\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.tensor([[-5.5, -2,1.1,3,0,5.5]])\n",
    "probabilities = nn.Softmax(dim = -1)\n",
    "output_tensor = probabilities(input_tensor)\n",
    "print(output_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b263fd",
   "metadata": {},
   "source": [
    "FORWARD PASS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7312f9c0",
   "metadata": {},
   "source": [
    "RESTART KERNEL : AFTER EACH TIME WE MAKE CHANGES IN THE utils.tensor_generator.tensor_gen()  \n",
    "\n",
    "gpt: \n",
    " Additional caveats\n",
    "If you do from utils import tensor_gen, then reload utils, your imported symbol tensor_gen won’t update unless you re-import it again.\n",
    "\n",
    "For complex projects, automatic reloaders like %autoreload in Jupyter (IPython) or tools like watchdog exist.\n",
    "\n",
    "In production code or big projects, use build/restart scripts or live-reload frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66d9d60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BINARY CLASSIFICATION FORWARD PASS\n",
    "input_bc = tensor_gen((5,6))    #(BATCH_SIZE, FEATURES)\n",
    "\n",
    "#create a binary classification nn\n",
    "\n",
    "binary_classifier = nn.Sequential(\n",
    "    nn.Linear(6,4),\n",
    "    nn.Linear(4,1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# PyTorch’s linear layers are batch-aware. SO ENTIRE BATCH IS PROCESSED SIMULTANEOUSLY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8d012a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5409],\n",
       "        [0.5758],\n",
       "        [0.5453],\n",
       "        [0.5362],\n",
       "        [0.5496]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_classifier(input_bc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1b9c25",
   "metadata": {},
   "source": [
    "THIS OUTPUT IS NOT BE MEANINGFUL UNTIL WE UPDATE THE WEIGHTS AND BIASES I.E. TRAIN THE MODEL USING BACK PROPAGATIION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de5a357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTI-CLASS CLASSIFICATION : FORWARD PASS \n",
    "n_classes = 3\n",
    "mc_input_tensor = tensor_gen((10,6))\n",
    "\n",
    "mc_classifier = nn.Sequential(\n",
    "    nn.Linear(6,4),\n",
    "    nn.Linear(4,n_classes),\n",
    "    # nn.Softmax()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aca3de74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3])\n",
      "tensor([[-0.0245, -0.2355,  0.2224],\n",
      "        [ 0.0959, -0.3584,  0.3160],\n",
      "        [ 0.0403, -0.2224,  0.3395],\n",
      "        [ 0.1978, -0.3101,  0.4453],\n",
      "        [ 0.0952, -0.2455,  0.3927],\n",
      "        [ 0.1199, -0.3498,  0.3317],\n",
      "        [-0.0785, -0.1890,  0.1912],\n",
      "        [-0.1020, -0.1691,  0.1773],\n",
      "        [ 0.0413, -0.2889,  0.3080],\n",
      "        [-0.0353, -0.1486,  0.2941]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mc_output = mc_classifier(mc_input_tensor)\n",
    "print(mc_output.shape)\n",
    "print(mc_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac96032",
   "metadata": {},
   "source": [
    "REGRESSION IN PYTORCH USING SEQUENTIAL MODULE \n",
    "\n",
    "EG: PREDICT THE WEIGHT OF THE ANIMALS USING THEIR FEATURES \n",
    "\n",
    "- DON'T USE ACTIVATION FUNCTION AT LAST AND LEAVE THE LOGITS AS IT IS WHICH CAN BE INTERPRETED AS THE (continuous value prediction by the model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86ba0677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0495],\n",
      "        [ 0.0383],\n",
      "        [ 0.1031],\n",
      "        [ 0.0186],\n",
      "        [ 0.1674],\n",
      "        [ 0.2515],\n",
      "        [ 0.0340],\n",
      "        [ 0.1080],\n",
      "        [ 0.1769],\n",
      "        [-0.0157]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "reg_model = nn.Sequential(\n",
    "    nn.Linear(6,4),\n",
    "    nn.Linear(4,1)\n",
    ")\n",
    "\n",
    "pred_weight = reg_model(mc_input_tensor)\n",
    "print(pred_weight)\n",
    "print(pred_weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12de440",
   "metadata": {},
   "source": [
    "USING LOSS FUNCTIONS TO ASSESS MODEL PREDICTIONS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ced6f497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "#ONE HOT ENCODING Y\n",
    "\n",
    "import torch.nn.functional as F \n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "print(F.one_hot(torch.tensor(2), num_classes= 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d30a7c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1667, dtype=torch.float64, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "scores = mc_output\n",
    "one_hot_target = one_hot_gen(batch_size=10, num_classes= 3)\n",
    "final_indices = one_hot_target.argmax(dim = 1) \n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "op = criterion(scores.double(), final_indices.long())\n",
    "# op = criterion(scores.double(), one_hot_target.double())\n",
    "print(op)\n",
    "# print(final_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2e0307",
   "metadata": {},
   "source": [
    "CROSS ENTROPY LOSS EXPECTS : CLASS LABELS AND NOT ONE HOT ENCODED ?!\n",
    "\n",
    "gemini: PyTorch nn.CrossEntropyLoss: This function is designed for efficiency and expects the target to be a class index, not a one-hot vector. For the given example, the correct target input would be a single integer 0, representing the index of the correct class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e59a54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.8222, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "'''PyTorch takes class_label indexes and not one hot encoded for loss computation\n",
    "HERE: pytorch is interpreting it as soft labels i.e target as probabilities'''\n",
    "\n",
    "scores = torch.tensor([-5.2,4.6, 0.8])\n",
    "# one_hot_target = torch.tensor([0.8,0.15,0.5])\n",
    "one_hot_target = torch.tensor([1,0,0])\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "# print(criterion(scores.double(), one_hot_target)) #RuntimeError: Expected floating point type for target with class probabilities, got Long\n",
    "print(criterion(scores.double(), one_hot_target.double()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ee8554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4782c6d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
